{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "#### Il faut installer enchant pour corriger des mots\n",
    "#### istaller nltk et aussi ajouer un wordnet en lancant  nltk.download()\n",
    "#### aussi j'ai remplace le ficher de stopswords par un autre version plus complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: <utf-8> -*-\n",
    "## pip install pyenchant, ntlk\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import csv \n",
    "import io\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readdata(filename):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    with io.open(train_fname, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            y.append(line[0])\n",
    "            X.append(line[5:])\n",
    "    return y,X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def repeatreplace(word):\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word=re.compile(r'(\\w*)(\\w)\\2(\\w*)').sub(r'\\1\\2\\3',word)\n",
    "    if repl_word!=word:\n",
    "        return replace(repl_word)\n",
    "    else:\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def repeatreplace(tokenList):\n",
    "    for i in range(len(tokenList)): \n",
    "        word=tokenList[i]\n",
    "        repl_word=re.compile(r'(\\w*)(\\w)\\2(\\w*)').sub(r'\\1\\2\\3',word)\n",
    "        if wordnet.synsets(word):\n",
    "            word=word\n",
    "        elif repl_word!=word:\n",
    "            word=replace(repl_word)\n",
    "        else:\n",
    "            word=repl_word\n",
    "        tokenList[i]=word\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removehtmltags(line):    \n",
    "    p=re.compile(r\"<.*?>\")\n",
    "    line = p.sub(' ', line)\n",
    "    line = line.replace('&nbsp;',' ')   \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removebadtoken(line):\n",
    "    rep = {'\\\\xa0': ' ', '\\\\xc2': ' ', '\\\\n': ' ', '\\r': ''}\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    line = pattern.sub(lambda m: rep[re.escape(m.group(0))], line)\n",
    "    return line   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Tokenization(line):\n",
    "    line=line.lower()\n",
    "    tokenlist = wordpunct_tokenize(line)\n",
    "    return tokenlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removestopwords(tokenList):\n",
    "    stopwordList=np.genfromtxt('stopwords.txt',dtype='str')\n",
    "    filteredWords = [w for w in tokenList if not w in stopwordList]\n",
    "    return filteredWords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commentnormalizer(line):\n",
    "    line = \"\".join([ch for ch in line if ch not in string.punctuation])\n",
    "    line = re.sub(\"\\\\d+(\\\\.\\\\d+)?\", \"NUM\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commentStemmer(tokenList):\n",
    "    stemmer = EnglishStemmer()\n",
    "    for i in range(len(tokenList)):\n",
    "        tokenList[i] = stemmer.stem(tokenList[i])\n",
    "    return tokenList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "def commentcorrect(tokenList):\n",
    "    dict_name=\"en\"\n",
    "    spell_dict=enchant.Dict(dict_name)\n",
    "    max_dist=2\n",
    "    for i in range(0,len(tokenList)):\n",
    "        if spell_dict.check(tokenList[i]):\n",
    "            tokenList[i]=tokenList[i]\n",
    "        else:\n",
    "            suggestions=spell_dict.suggest(tokenList[i])\n",
    "            if suggestions and edit_distance(tokenList[i],suggestions[0])<=max_dist:\n",
    "                tokenList[i]=suggestions[0]\n",
    "            else:\n",
    "                tokenList[i]=tokenList[i]\n",
    "    return tokenList        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shit', 'language']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentcorrect([\"sh?t\",\"languege\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## badwords in the bad words list\n",
    "import string \n",
    "def encountebadwords(tokenList):  \n",
    "    \n",
    "    badList = []\n",
    "    with io.open('./badwords.txt', newline = '\\n') as f:\n",
    "         for line in f:     \n",
    "            badList.append(line[:len(line)-1])\n",
    "    badList=commentcorrect(badList)\n",
    "    badList=repeatreplace(badList)\n",
    "    c=0\n",
    "    for w in badList:\n",
    "        if wordnet.synsets(w):\n",
    "            print \"true\",w\n",
    "            c=c+1 \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print w\n",
    "    print c\n",
    "    print len(badList)\n",
    "    count=0\n",
    "    for w in tokenList:\n",
    "        if w in badList:\n",
    "            count=count+1\n",
    "    return  count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'replace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2bd8310b9dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencountebadwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fuck\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"shit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-3c87f3db2e7d>\u001b[0m in \u001b[0;36mencountebadwords\u001b[0;34m(tokenList)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbadList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbadList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommentcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbadList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepeatreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbadList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b4dbd67d5edf>\u001b[0m in \u001b[0;36mrepeatreplace\u001b[0;34m(tokenList)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrepl_word\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepl_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'replace' is not defined"
     ]
    }
   ],
   "source": [
    "encountebadwords([\"fuck\",\"shit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exemple pour utiliser les fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y,X=readdata(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'imagin', u'e', u'e', u'sanction', u'e', u'forev', u'hear', u'ea', u'regul', u'e', u'hide', u'pretenc', u'friend', u'nuclear', u'energi', u'.', u'2', u'day', u'e', u'.', u'.', u'inspector', u'e', u'quit', u'kill', u'civilian', u'e', u'respect', u'border', u'right', u'neighbour', u'countri', u'e', u'e', u'e', u'shut', u'nuclear', u'plant', u'e', u'monitor', u'system', u'fanci', u'e', u'water', u'treatment', u'plant', u'earli', u'warn', u'sandstorm', u'system', u'traffic', u'light', u'major', u'citi', u'...', u'..(', u'inki', u'finger', u'lip', u'edg', u'e', u'teenag', u'revolt', u'toppl', u'regim', u'...', u'disconnect', u'...', u'facebok', u'....', u'buwhahjahahaha', u'.\"\"\"']\n",
      "[u'imagin', u'e', u'e', u'sanction', u'e', u'forev', u'hear', u'ea', u'regul', u'e', u'hide', u'pretenc', u'friend', u'nuclear', u'energi', u'.', u'2', u'day', u'e', u'.', u'.', u'inspector', u'e', u'quit', u'kill', u'civilian', u'e', u'respect', u'border', u'right', u'neighbour', u'countri', u'e', u'e', u'e', u'shut', u'nuclear', u'plant', u'e', u'monitor', u'system', u'fanci', u'e', u'water', u'treatment', u'plant', u'earli', u'warn', u'sandstorm', u'system', u'traffic', u'light', u'major', u'citi', u'...', u'..(', u'inki', u'finger', u'lip', u'edg', u'e', u'teenag', u'revolt', u'toppl', u'regim', u'...', u'disconnect', u'...', u'facebok', u'....', u'buwhahjahahaha', u'.\"\"\"']\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0,len(X)):\n",
    "for i in range(0,2):\n",
    "    lines=removehtmltags(X[i])\n",
    "    line=removebadtoken(line)\n",
    "    lines=commentnormalizer(line)\n",
    "    lines=Tokenization(line) \n",
    "    lines=removestopwords(lines)\n",
    "    lines=repeatreplace(lines)\n",
    "    lines=commentcorrect(lines)\n",
    "    lines=commentStemmer(lines)\n",
    "    print lines\n",
    "\n",
    "\n",
    "#     print len(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
