{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "#### Il faut installer enchant pour corriger des mots\n",
    "#### istaller nltk et aussi ajouer un wordnet en lancant  nltk.download()\n",
    "#### aussi j'ai remplace le ficher de stopswords par un autre version plus complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: <utf-8> -*-\n",
    "## pip install pyenchant, ntlk\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import csv \n",
    "import io\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test =[1,2,3]\n",
    "print 1 in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readdata(filename):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    with io.open(filename, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            y.append(line[0])\n",
    "            X.append(line[5:])\n",
    "    return y,X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordreplace(word):\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word=re.compile(r'(\\w*)(\\w)\\2(\\w*)').sub(r'\\1\\2\\3',word)\n",
    "    if repl_word!=word:\n",
    "        return wordreplace(repl_word)\n",
    "    else:\n",
    "        return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def repeatreplace(tokenList):\n",
    "    for i in range(len(tokenList)): \n",
    "        word=tokenList[i]\n",
    "        word=wordreplace(word)\n",
    "        tokenList[i]=word\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck', 'shit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeatreplace([\"fuuuuck\",\"shiittt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removehtmltags(line):    \n",
    "    p=re.compile(r\"<.*?>\")\n",
    "    line = p.sub(' ', line)\n",
    "    line = line.replace('&nbsp;',' ')   \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removebadtoken(line):\n",
    "    rep = {'\\\\xa0': ' ', '\\\\xc2': ' ', '\\\\n': ' ', '\\r': ''}\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    line = pattern.sub(lambda m: rep[re.escape(m.group(0))], line)\n",
    "    return line   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Tokenization(line):\n",
    "    line=line.lower()\n",
    "    tokenlist = wordpunct_tokenize(line)\n",
    "    return tokenlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removestopwords(tokenList):\n",
    "    stopwordList=np.genfromtxt('stopwords.txt',dtype='str')\n",
    "    filteredWords = [w for w in tokenList if not w in stopwordList]\n",
    "    return filteredWords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commentnormalizer(line):\n",
    "    line = \"\".join([ch for ch in line if ch not in string.punctuation])\n",
    "    line = re.sub(\"\\\\d+(\\\\.\\\\d+)?\", \"NUM\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commentStemmer(tokenList):\n",
    "    stemmer = EnglishStemmer()\n",
    "    for i in range(len(tokenList)):\n",
    "        tokenList[i] = stemmer.stem(tokenList[i])\n",
    "    return tokenList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "def commentcorrect(tokenList):\n",
    "    dict_name=\"en\"\n",
    "    spell_dict=enchant.Dict(dict_name)\n",
    "    max_dist=2\n",
    "    for i in range(0,len(tokenList)):\n",
    "        if spell_dict.check(tokenList[i]):\n",
    "            tokenList[i]=tokenList[i]\n",
    "        else:\n",
    "            suggestions=spell_dict.suggest(tokenList[i])\n",
    "            if suggestions and edit_distance(tokenList[i],suggestions[0])<=max_dist:\n",
    "                tokenList[i]=suggestions[0]\n",
    "            else:\n",
    "                tokenList[i]=tokenList[i]\n",
    "    return tokenList        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shit', 'language']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentcorrect([\"sh?t\",\"languege\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## badwords in the bad words list\n",
    "badList = []\n",
    "def encountebadwords(tokenList):  \n",
    "    \n",
    "\n",
    "    with io.open('badwords.txt', newline = '\\n') as f:\n",
    "         for line in f:     \n",
    "            badList.append(line[:len(line)-1])\n",
    "#     badList=commentcorrect(badList)\n",
    "#     badList=repeatreplace(badList)\n",
    "#     c=0\n",
    "#     for w in badList:\n",
    "#         if wordnet.synsets(w):\n",
    "#             print \"true\",w\n",
    "#             c=c+1             \n",
    "#         else:\n",
    "#             print w\n",
    "#     print c\n",
    "#     print len(badList)\n",
    "    count=0\n",
    "    for w in tokenList:\n",
    "        if w in badList:\n",
    "            count=count+1\n",
    "    return  count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encountebadwords([\"fuck\",\"shit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exemple pour utiliser les fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processingtrain(filename):\n",
    "    y,X=readdata(filename)\n",
    "    Count=[]\n",
    "    for i in range(0,len(X)):\n",
    "        lines=removehtmltags(X[i])\n",
    "        lines=removebadtoken(lines)\n",
    "        lines=commentnormalizer(lines)\n",
    "        lines=Tokenization(lines) \n",
    "        lines=removestopwords(lines)\n",
    "        lines=repeatreplace(lines)\n",
    "        lines=commentcorrect(lines)\n",
    "        lines=commentStemmer(lines)\n",
    "        countbad =encountebadwords(lines)\n",
    "        Count.append(countbad)\n",
    "        X[i]=lines\n",
    "    return y,X,Count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y,X,C=processingtrain(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTestFile(test_fname):\n",
    "    with io.open(test_fname, encoding = 'utf-8') as f:\n",
    "         for line in f: \n",
    "            line = line.replace(\"\\\\xa0\", \"\")\n",
    "            line = line.replace(\"\\\\n\", \"\")\n",
    "            line = line.lower()\n",
    "            test.append(line)\n",
    "    print test[0]\n",
    "    return test\n",
    "\n",
    "def processingTest(test_fname):\n",
    "    X = test_fname(test_fname)\n",
    "    Count=[]\n",
    "    for i in range(0,len(X)):\n",
    "        lines=removehtmltags(X[i])\n",
    "        lines=removebadtoken(lines)\n",
    "        lines=commentnormalizer(lines)\n",
    "        lines=Tokenization(lines) \n",
    "        lines=removestopwords(lines)\n",
    "        lines=repeatreplace(lines)\n",
    "        lines=commentcorrect(lines)\n",
    "        lines=commentStemmer(lines)\n",
    "        countbad =encountebadwords(lines)\n",
    "        Count.append(countbad)\n",
    "        X[i]=lines\n",
    "    return X,Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'1', u'0'] [0, 1, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "[u'imagin', u'sanction', u'forev', u'hear', u'ea', u'regul', u'hide', u'pretenc', u'friend', u'nuclear', u'energi', u'um', u'day', u'inspector', u'quit', u'kill', u'civilian', u'respect', u'border', u'right', u'neighbour', u'countri', u'shut', u'nuclear', u'plant', u'monitor', u'system', u'fanci', u'water', u'treatment', u'plant', u'earli', u'warn', u'sandstorm', u'system', u'traffic', u'light', u'major', u'citi', u'inki', u'finger', u'lip', u'edg', u'teenag', u'revolt', u'toppl', u'regim', u'disconnect', u'facebok', u'buwhahjahahaha']\n"
     ]
    }
   ],
   "source": [
    "print y[:10],C[:10]\n",
    "print X[0]\n",
    "C = np.array(C).reshape(len(C), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4415, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.69429348,  0.69429348,  0.69340585])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = SVC(kernel='linear')\n",
    "print C.shape\n",
    "cross_val_score(clf, C, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def words2Vectors(wordList, data):\n",
    "#     vec = [0] * len(wordList)\n",
    "#     for word in data:\n",
    "#         if word in wordList:\n",
    "#             vec[wordList.index(word)] = vec[wordList.index(word)] + 1 // +1\n",
    "#     return vec\n",
    "\n",
    "# def createWordList(wordContent):\n",
    "#     # no word occur twice\n",
    "#     wordSet = set()\n",
    "#     for word in wordContent:\n",
    "#         if word not in wordSet:\n",
    "#             wordSet.add(word)\n",
    "#     return list(wordSet)\n",
    "\n",
    "    \n",
    "def words2Vectors(wordList, data):\n",
    "    vec = [0] * len(wordList)\n",
    "    for word in data:\n",
    "        if word in wordList:\n",
    "            vec[wordList.index(word)] = vec[wordList.index(word)] + 1  + (word in badList) \n",
    "            # count bad words twice\n",
    "    return vec\n",
    "\n",
    "def createWordList(X):\n",
    "    wordContent = []\n",
    "    wordCount = dict()\n",
    "    for line in X:\n",
    "        for word in line:\n",
    "            if word in wordCount.keys():\n",
    "                wordCount[word] = wordCount[word] + 1 + (word in badList)\n",
    "            else:\n",
    "                wordCount[word] = 1 + (word in badList)\n",
    "\n",
    "    return wordCount.keys(), wordCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordList, wordCount = createWordList(X)\n",
    "vectors = []\n",
    "for x in X:\n",
    "    vectors.append(words2Vectors(wordList, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "clf = SVC(kernel='linear', gamma=2)\n",
    "select = SelectKBest(chi2)\n",
    "pipe = Pipeline(steps=[('chi2', select), ('SVM', clf)])\n",
    "para = {\"chi2__k\" : np.arange(50, 3100, 200)}\n",
    "estimator = GridSearchCV(pipe,para)\n",
    "estimator.fit(X_train_tf.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chi2__k': 50}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(vectors)\n",
    "X_train_tf = tf_transformer.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"tf\",X_train_tf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('SVM', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=2, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': array([  50,  250,  450,  650,  850, 1050, 1250, 1450, 1650, 1850, 2050,\n",
       "       2250, 2450, 2650, 2850, 3050])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pca = PCA()\n",
    "clf = SVC(kernel='linear', gamma=2)\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca), ('SVM', clf)])\n",
    "n_components = np.arange(50, 3100, 200)\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components))\n",
    "estimator.fit(X_train_tf.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799093997735\n",
      "{'pca__n_components': 2450}\n"
     ]
    }
   ],
   "source": [
    "print estimator.best_score_\n",
    "print estimator.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# X_train_tf = sp.sparse.hstack((X_train_tf, char_counts))\n",
    "\n",
    "# This dataset is way to high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2450)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "# election = SelectKBest(k=500)\n",
    "\n",
    "# combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "combined_features = FeatureUnion([(\"pca\", pca)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X_train_tf.toarray(), y).transform(X_train_tf.toarray())\n",
    "# test_features = combined_features.transform(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kernel': ['linear'], 'C': [1, 2, 3, 4, 5, 6, 7, 8]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 2, 3, 4,5,6,7,8]}, {'kernel': ['linear'], 'C': [1, 2, 3, 4,5,6,7,8]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters)\n",
    "clf.fit(X_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796149490374\n",
      "{'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "print clf.best_score_\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x127ab40d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEzZJREFUeJzt3HFsnHd9x/HPJ3ajuR3pamLSLYFko+3IEFnbaV5Gq+qg\nI3XRpgKTRlINjUqwNJpppf3RhGkonoQ0kj+mgaqkCmQQpsXRRpfRggoug9OUjMzeIE2ZLyRQOWsp\nxA5jaKD+kWbf/eFLONt3tp97Hvt893u/pJPv8X3v9/s998Sfe/z1PXFECACQllWtXgAAYPkR/gCQ\nIMIfABJE+ANAggh/AEgQ4Q8ACSok/G0ftn3R9pkGjz9o+7nq7YTttxQxLwCgOUWd+X9a0n3zPP6C\npHsi4tclfVTSJwuaFwDQhO4iBomIE7Y3zvP4qZrNU5LWFzEvAKA5rej5f0DSMy2YFwBQVciZ/2LZ\nfpukhyTdvZzzAgBmWrbwt71F0iFJAxHxo3nq+M+GACCjiHCW+iLbPq7e5j5gv0HSk5LeFxHfXWig\niOjI2969e1u+BvaP/WP/Ou/WjELO/G0flVSS9Frb/yVpr6TV0zkehyR9RFKvpAO2LelyRPQXMTcA\nILuiPu3z4AKPf1DSB4uYCwCQH1f4LqNSqdTqJSwp9q+9sX9pcbP9oqViO1bamgBgJbOtaOEffAEA\nbYLwB4AEEf4AkCDCHwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEkT4A0CC\nCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggoJf9uHbV+0fWaemk/YPm/7tO3b\ni5gXANCcos78Py3pvkYP2r5f0hsj4lZJOyU9UdC8bcP2tVvR9UtV29fXJ9vq6+tbsHb//v264447\ntH///gVrK5WKjhw5okqlsmDtyZMntXfvXp08eXLBWkmamprS2NiYpqamCl1HFlnWkKUWzeN1riMi\nCrlJ2ijpTIPHnpD03prtiqR1DWqj00hdIfWEdGv1qwurXwm111//CzNqb7hhTcPawcFHq7W3hdQT\ng4OPNKx9xzvunzHutm33N6yNiDh69Fj09PTGjTfeGT09vXH06LFC1pFFljVkqUXzUnidq7mZLbOz\nPqHhQPOH/9OS3lqz/RVJdzaoXaKXpzUkVUPmuZCi+rUnGu1nlvqlql27dm3d2rVr186p3bdvX93a\nffv2zakdHx+vWzs+Pj6n9sSJE3VrT5w4Ufd1m5ycjJ6e3hn1PT29MTk5mWsdWWRZQ5ZaNC+V17mZ\n8O9e+t8tshsaGrp2v1QqqVQqtWwtxdggaUv1/hZJ6yV9p6D64msvXbok6dY5tZcuza0dHh6uO+7w\n8LAee+yxGbWjo6OSXj+rdoNGR0e1efPmGbUjIyN1xx0ZGdFdd901Zx0TExNavXqTXnnlZ/XXXbdR\nExMTc9pWWdaRRZY1ZKlF8zr1dS6XyyqXy/kGyfpu0eimbG2fs0qk7SPO/K/hzL+5WjQvlddZLW77\nbJL0fIPH3inpi9X7WyWdmmecJXp5WkdyNWxuicX1/BdfvxJqb7hhzYza+Xv+j0RtH3++Xvu2bffP\nGHexPf81a+5YRM9/8evIIssastSieSm8zs2Ev6efl4/to5JKkl4r6aKkvZJWVxd0qFrzuKQBST+V\n9FBEfKPBWFHEmlaa2k/XLGb/stQvVW1fX58uXbqktWvXLvgpif3792t4eFg7duyY0+6ZrVKpaHR0\nVP39/Qu2WU6ePKmRkRFt27atbrtntqmpKU1MTGjTpk0L/lqfZR1ZZFlDllo0r9NfZ9uKiMV9lPDq\nc1Za0HZq+APAUmkm/LnCFwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEkT4\nA0CCCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ4Q8ACSL8ASBBhD8A\nJIjwB4AEFRL+tgdsn7V9zvbuOo+vsf2U7dO2n7f9/iLmBQA0xxGRbwB7laRzku6V9LKkMUnbI+Js\nTc2HJa2JiA/bXivp25LWRcSrdcaLvGsCgJTYVkQ4y3OKOPPvl3Q+Ii5ExGVJxyQ9MKsmJL2mev81\nkn5YL/gBAMujiPBfL+nFmu2Xqt+r9bikX7P9sqTnJD1awLwAgCZ1L9M890n6ZkS83fYbJT1re0tE\n/KRe8dDQ0LX7pVJJpVJpWRYJAO2gXC6rXC7nGqOInv9WSUMRMVDd3iMpImJfTc0XJP1lRJysbv+z\npN0R8e91xqPnDwAZtKrnPybpFtsbba+WtF3SU7NqLkj6neoi10m6TdILBcwNAGhC7rZPRFyxPShp\nRNNvJocjomJ75/TDcUjSRyV9xvaZ6tMei4j/zjs3AKA5uds+RaPtAwDZtKrtAwBoM4Q/ACSI8AeA\nBBH+AJAgwh8AEkT4A0CCCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ\n4Q8ACSL8ASBBhD8AJIjwB4AEEf4AkCDCHwASRPgDQIIIfwBIUCHhb3vA9lnb52zvblBTsv1N29+y\n/bUi5gUANMcRkW8Ae5Wkc5LulfSypDFJ2yPibE3NjZL+VdK2iPie7bURcanBeJF3TQCQEtuKCGd5\nThFn/v2SzkfEhYi4LOmYpAdm1Two6cmI+J4kNQp+AMDyKCL810t6sWb7per3at0mqdf212yP2X5f\nAfMCAJrUvYzz3Cnp7ZJukPR121+PiO/UKx4aGrp2v1QqqVQqLcMSAaA9lMtllcvlXGMU0fPfKmko\nIgaq23skRUTsq6nZLennIuIvqtufkvRMRDxZZzx6/gCQQat6/mOSbrG90fZqSdslPTWr5vOS7rbd\nZft6Sb8lqVLA3ACAJuRu+0TEFduDkkY0/WZyOCIqtndOPxyHIuKs7S9LOiPpiqRDETGed24AQHNy\nt32KRtsHALJpVdsHANBmCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ\n4Q8ACSL8ASBBhD8AJIjwB4AEEf4AkCDCHwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+\nAJCgQsLf9oDts7bP2d49T91v2r5s+z1FzAsAaE7u8Le9StLjku6T9GZJO2y/qUHdxyR9Oe+cAIB8\nijjz75d0PiIuRMRlScckPVCn7kOSPidpsoA5AQA5FBH+6yW9WLP9UvV719j+JUnvioiDklzAnACA\nHLqXaZ6/llT7t4B53wCGhoau3S+VSiqVSkuyKABoR+VyWeVyOdcYjoh8A9hbJQ1FxEB1e4+kiIh9\nNTUvXL0raa2kn0r644h4qs54kXdNAJAS24qITF2VIsK/S9K3Jd0r6fuSRiXtiIhKg/pPS3o6Iv6x\nweOEPwBk0Ez45277RMQV24OSRjT9N4TDEVGxvXP64Tg0+yl55wQA5JP7zL9onPkDQDbNnPlzhS8A\nJIjwB4AEEf4AkCDCHwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEkT4A0CC\nCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ4Q8ACSok/G0P2D5r+5zt\n3XUef9D2c9XbCdtvKWJeAEBzHBH5BrBXSTon6V5JL0sak7Q9Is7W1GyVVImIH9sekDQUEVsbjBd5\n1wQAKbGtiHCW5xRx5t8v6XxEXIiIy5KOSXqgtiAiTkXEj6ubpyStL2BeAECTigj/9ZJerNl+SfOH\n+wckPVPAvACAJnUv52S23ybpIUl3z1c3NDR07X6pVFKpVFrSdQFAOymXyyqXy7nGKKLnv1XTPfyB\n6vYeSRER+2bVbZH0pKSBiPjuPOPR8weADFrV8x+TdIvtjbZXS9ou6alZC3uDpoP/ffMFPwBgeeRu\n+0TEFduDkkY0/WZyOCIqtndOPxyHJH1EUq+kA7Yt6XJE9OedGwDQnNxtn6LR9gGAbFrV9gEAtBnC\nHwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEkT4A0CCCH8ASBDhDwAJIvwB\nIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ4Q8ACSL8ASBBhD8AJKiQ8Lc9YPus7XO2dzeo\n+YTt87ZP2769iHkBAM3JHf62V0l6XNJ9kt4saYftN82quV/SGyPiVkk7JT2Rd952Y/varej6LLXd\n3d2yre7u7gVrd+3apZtvvlm7du1asPbgwYO65557dPDgwQVrK5WKjhw5okqlsmDt1NSUxsbGNDU1\ntWDtUmrHNQPziohcN0lbJT1Ts71H0u5ZNU9Iem/NdkXSugbjRaeRukLqCenW6lcXVr90tatn1Nrd\nDWtvumndjNre3r6GtYODj1ZrbwupJwYHH2lYe/Tosejp6Y0bb7wzenp64+jRYw1rl1I7rhlpqeZm\ntuzO+oQ5A0i/L+lQzfYfSvrErJqnJb21Zvsrku5sMN6SvUCtIKkaHM+FFNWvPdFoP7PUZ6nt6uqq\nW9vV1TWn9uGHH65b+/DDD8+pPXDgQN3aAwcOzKkdHx+vWzs+Pj6ndnJyMnp6emfU9vT0xuTkZN3X\nbam045qRnmbCf+Hf/VtgaGjo2v1SqaRSqdSytRRjg6Qt1ftbJK2X9J2C6hdXe+XKlbq1V67MrT1+\n/Hjd2uPHj89p6wwPD9etHR4entMuGh0dlfT6WbUbNDo6qs2bN8+onZiY0OrVm/TKKz+rve66jZqY\nmFBfX1/9l2IJtOOa0fnK5bLK5XK+QbK+W8y+abrt86Wa7cW0fc4qkbaPOPO/ph3PottxzUiPWtT2\n6dL0qeZGSaslnZa0eVbNOyV9MX72ZnFqnvGW8CVqDcnVALklFtfzX3z9UtXa3TNq5+v59/b2zaid\nv+f/SNT+fWAx/fM1a+5occ+//daMtDQT/p5+Xj62ByR9XNOfHjocER+zvbO6oEPVmsclDUj6qaSH\nIuIbDcaKIta00tR+Emcx+5elPkttd3e3rly5oq6uLr366qvz1u7atUvHjx/Xu9/97gU/xXPw4EEN\nDw9rx44dC346qFKpaHR0VP39/XNaJ7NNTU1pYmJCmzZtamnrpB3XjHTYVkQs7qOEV5+z0oK2U8Mf\nAJZKM+HPFb4AkCDCHwASRPgDQIIIfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEkT4A0CC\nCH8ASBDhDwAJIvwBIEGEPwAkiPAHgAQR/gCQIMIfABJE+ANAggh/AEgQ4Q8ACSL8ASBBucLf9k22\nR2x/2/aXbd9Yp2aD7a/a/k/bz9t+JM+cAID88p7575H0lYj4VUlflfThOjWvSvrTiHizpN+W9Ce2\n35Rz3rZULpdbvYQlxf61N/YvLXnD/wFJR6r3j0h61+yCiPhBRJyu3v+JpIqk9TnnbUud/o+P/Wtv\n7F9a8ob/6yLiojQd8pJeN1+x7U2Sbpf0bznnBQDk0L1Qge1nJa2r/ZakkPTndcpjnnF+XtLnJD1a\n/Q0AANAijmiY1ws/2a5IKkXERds3S/paRGyuU9ct6QuSnomIjy8wZvMLAoBERYSz1C945r+ApyS9\nX9I+SX8k6fMN6v5G0vhCwS9l3wEAQHZ5z/x7Jf29pNdLuiDpDyLif2z/oqRPRsTv2r5L0r9Iel7T\nbaGQ9GcR8aXcqwcANCVX+AMA2tOKu8LX9l7bL9n+RvU20Oo1FcH2gO2zts/Z3t3q9RTN9oTt52x/\n0/Zoq9eTl+3Dti/aPlPzvQUvamwXDfavI372Gl1Y2inHr87+faj6/UzHb8Wd+dveK+l/I+KvWr2W\notheJemcpHslvSxpTNL2iDjb0oUVyPYLkn4jIn7U6rUUwfbdkn4i6bMRsaX6vX2SfhgR+6tv4DdF\nxJ5WrrNZDfavI372qh8+uTkiTlc/Zfgfmr4m6SF1wPGbZ//eqwzHb8Wd+Vd12h99+yWdj4gLEXFZ\n0jFNH6xOYq3cf0+ZRcQJSbPfyBa8qLFdNNg/qQN+9hpcWLpBHXL8FrhwdtHHb6X+sA7aPm37U+36\nq9ks6yW9WLP9kjrvKueQ9KztMdsfbPVilkimixrbVEf97NVcWHpK0rpOO351Lpxd9PFrSfjbftb2\nmZrb89WvvyfpgKRfiYjbJf1AUlv/CpqQuyLiTknv1PT/33R3qxe0DFZWzzS/jvrZq3Nh6ezj1dbH\nr87+ZTp+eT/n35SIeMciSz8p6emlXMsy+Z6kN9Rsb6h+r2NExPerX6dsH9d0q+tEa1dVuIu219Vc\n1DjZ6gUVKSKmajbb+mevemHp5yT9bURcvf6oY45fvf3LevxWXNunelCueo+kb7VqLQUak3SL7Y22\nV0varukL5DqC7eurZyGyfYOkbeqM42bN7KFevahRmv+ixnYxY/867Gev3oWlnXT85uxf1uO3Ej/t\n81lN97D+T9KEpJ1X+3TtrPqxq49r+g33cER8rMVLKoztX5Z0XNO/RndL+rt23z/bRyWVJL1W0kVJ\neyX9k6R/0KyLGlu1xjwa7N/b1AE/e40uLJU0qjoXpbZqnc2aZ/8eVIbjt+LCHwCw9FZc2wcAsPQI\nfwBIEOEPAAki/AEgQYQ/ACSI8AeABBH+AJAgwh8AEvT/aVb/oJF2DJgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11afce590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def words2Vectors(wordList, data):\n",
    "    vec = [0] * len(wordList)\n",
    "    for word in data:\n",
    "        if word in wordList:\n",
    "            vec[wordList.index(word)] = vec[wordList.index(word)] + 1 // +1\n",
    "    return vec\n",
    "\n",
    "def createWordList(wordContent):\n",
    "    # no word occur twice\n",
    "    wordSet = set()\n",
    "    for word in wordContent:\n",
    "        if word not in wordSet:\n",
    "            wordSet.add(word)\n",
    "    return list(wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y,X=readdata(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(0,len(X)):\n",
    "# for i in range(0,1):\n",
    "#     lines=removehtmltags(X[i])\n",
    "#     line=removebadtoken(line)\n",
    "#     lines=commentnormalizer(line)\n",
    "#     lines=Tokenization(line) \n",
    "#     lines=removestopwords(lines)\n",
    "#     lines=repeatreplace(lines)\n",
    "#     lines=commentcorrect(lines)\n",
    "#     lines=commentStemmer(lines)\n",
    "#     countbad=encountebadwords(lines)\n",
    "#     print len(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
